{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import IPy\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import cryptography\n",
    "import pprint\n",
    "from cryptography.hazmat._oid import ObjectIdentifier\n",
    "import pyarrow as pa\n",
    "os.environ[\"ARROW_LIBHDFS_DIR\"] = \"/usr/local/hadoop/lib/native\"\n",
    "\n",
    "# Find Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# PySpark imports\n",
    "import pyspark\n",
    "import pyspark.sql.functions as psf\n",
    "import pyspark.sql.types as pst\n",
    "\n",
    "\n",
    "# A helper to kinit, keytab-based\n",
    "def kinit_helper(principal, keytab):\n",
    "    \n",
    "    kinit_cmd = \"kinit -p {} -k -t {} -l 7d -r 7d\".format(principal, keytab)\n",
    "    \n",
    "    # Call subprocess to execute cmd\n",
    "    process = subprocess.Popen(kinit_cmd, stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, executable=\"/bin/bash\")\n",
    "    output, error = process.communicate()\n",
    "    \n",
    "    stdout_str = error.decode(\"utf-8\")\n",
    "    if len(stdout_str) > 0:\n",
    "        logger.info(stdout_str)\n",
    "    \n",
    "    stderr_str = error.decode(\"utf-8\")\n",
    "    if len(stderr_str) > 0:\n",
    "        logger.error(stderr_str)\n",
    "        \n",
    "# A helper to get a (pyarrow) hdfs client\n",
    "def hdfs_fs_helper(principal, host=\"default\"):\n",
    "    \n",
    "    # Create a HDFS connection\n",
    "    return pa.hdfs.connect(host=u\"{}\".format(host), port=0, user=u\"{}\".format(principal), kerb_ticket=u\"/tmp/krb5cc_{}\".format(os.getuid()), driver=u\"libhdfs\")\n",
    "\n",
    "# Helper to filter out LE certs\n",
    "def filter_LE(obj):\n",
    "    if not obj:\n",
    "        return False\n",
    "    return obj.asDict().get(\"CN\") == \"Let's Encrypt Authority X3\"\n",
    "\n",
    "# Get count of items in array\n",
    "slen = psf.udf(lambda s: len(s), pst.IntegerType())\n",
    "\n",
    "\n",
    "filter_LE_udf = psf.udf(filter_LE, pst.BooleanType())\n",
    "\n",
    "# setup\n",
    "# Set up some variables to set up the connection. Some information is redacted for security and privacy.\n",
    "APP_NAME = \"LE-analysis\"\n",
    "\n",
    "EXTERNAL_DRIVER_IP = \"\" # Your external IP address, for callback connections to the Spark driver\n",
    "OI_KRB_PRINCIPAL = \"\"        # Your assigned username, i.e., Kerberos principal\n",
    "\n",
    "OI_KRB_KEYTAB    = os.path.join(     # Your Kerberos keytab\n",
    "    os.path.expanduser(\"~\"),\n",
    "    \"{}.keytab\".format(OI_KRB_PRINCIPAL)\n",
    ")\n",
    "\n",
    "## DO NOT CHANGE THE CONFIGURATION BELOW UNLESS YOU KNOW WHAT YOU ARE DOING\n",
    "spark_conf = pyspark.SparkConf().setAppName(APP_NAME\n",
    ").setMaster(\"yarn\").set(\"spark.scheduler.pool\", \"root.users.{}\".format(OI_KRB_PRINCIPAL)\n",
    ").set(\"spark.submit.deployMode\", \"client\"\n",
    ").set(\"spark.authenticate\", \"true\"\n",
    ").set(\"spark.sql.parquet.binaryAsString\", \"true\"\n",
    ").set(\"spark.network.crypto.enabled\", \"true\"\n",
    ").set(\"spark.driver.host\", EXTERNAL_DRIVER_IP\n",
    ").set(\"spark.driver.bindAddress\", \"0.0.0.0\"\n",
    ").set(\"spark.driver.port\", \"33007\").set(\"spark.blockManager.port\", \"33023\").set(\"spark.ui.port\", \"33039\"\n",
    ").set(\"spark.driver.cores\",\"2\").set(\"spark.driver.memory\",\"4G\"\n",
    ").set(\"spark.executor.cores\", \"5\").set(\"spark.executor.memory\", \"12G\").set(\"spark.executor.memoryOverhead\", \"4G\"\n",
    ").set(\"spark.dynamicAllocation.enabled\", \"true\").set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "\n",
    "# The locations where the raw parquet data is stored. To be loaded later\n",
    "OI_CTLOGS_BASE_19 = \"/user/openintel/ct-logs/type=warehouse/name=LetsEncryptOak-2019\"\n",
    "OI_CTLOGS_BASE_20 = \"/user/openintel/ct-logs/type=warehouse/name=LetsEncryptOak-2020\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop/clear context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache\n",
    "\n",
    "sqlc.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinit_helper(OI_KRB_PRINCIPAL, OI_KRB_KEYTAB)\n",
    "\n",
    "# SparkContext\n",
    "sc = pyspark.SparkContext(conf=spark_conf)\n",
    "\n",
    "# SQLContext\n",
    "sqlc = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Context\n",
    "ct_logs19_df = sqlc.read.format(\"parquet\").load(OI_CTLOGS_BASE_19)\n",
    "ct_logs20_df = sqlc.read.format(\"parquet\").load(OI_CTLOGS_BASE_20)\n",
    "# Use union to combine both datasets.\n",
    "ct_logs_df = ct_logs19_df.union(ct_logs20_df)\n",
    "# Set up dataframe that is fitlered to only contain certificates issued by Let's Encrypt\n",
    "ct_logs_LE_df = ct_logs_df.where(filter_LE_udf(psf.col(\"leaf_cert.issuer\")))\n",
    "# ct_logs_LE_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate certificates\n",
    "Further research required.\n",
    "It shows that a load of the certificates found in the logs are duplicates. In the context that they cover the same domains and have the same not_before timestamp.\n",
    "Up to 66 times even.\n",
    "Further research must show why this is and further characteristics within this data. Noteworthy is that these numbers often are often even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of certificates (after filtering)\n",
    "ct_logs_LE_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the certs are duplicates. Covering the same Common Names and having the same not_before timestamp.\n",
    "\n",
    "ct_logs_LE_df.withColumn(\"not_before\", psf.col(\"leaf_cert.not_before\").cast(pst.DecimalType(12, 0))).groupBy(\"all_domains\", \"not_before\").count().groupBy(\"count\").count().sort(psf.desc(\"count\")).show(1000, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What proportion of certificates are being renewed\n",
    "\n",
    "This looks into how often a certificate is being renewed. Since many certificates seem to be renewed after very short timeframes (minutes-hours) this will look at minimum intervals of 7 days.\n",
    "This interval is constructed as follows. Get the first certificate for the domains and grab the timestamp. For every certificate after this check that it has been at least 7 days since the first one. When one is found reset the timestamp and repeat this process until all certificates have been checked.\n",
    "\n",
    "Pseudo-code\n",
    "```python\n",
    "function filter(certificates):\n",
    "    filtered_certificates = [certificates[0]] # list containing the first certificate which will get appended with all following certificates\n",
    "    timestamp = certificates[0].timestamp\n",
    "    for certificate in certificates[1:]: # for each certificate after the first one\n",
    "        if certificate.timestamp + 7_days > timestamp:\n",
    "            timestamp = certificate.timestamp\n",
    "            filtered_certificates.append(certificate)\n",
    "    return filtered_certificates\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the udf to filter out certificates that do not have enough time between them\n",
    "\n",
    "#month\n",
    "interval = 60*60*24*30\n",
    "\n",
    "def filter_duration(timestamps):\n",
    "    timestamps.sort()\n",
    "    result = [timestamps[0]]\n",
    "    last = timestamps[0]\n",
    "    for o in timestamps[1:]:\n",
    "        if last+interval < o:\n",
    "            result.append(o)\n",
    "            last = o\n",
    "    return result\n",
    "\n",
    "filter_duration_udf = psf.udf(lambda y: filter_duration(y), pst.ArrayType(pst.FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the dataframe for the filtered certificates. Make persistant since this is often re-used.\n",
    "filtered_df = ct_logs_LE_df.groupBy('all_domains').agg(psf.collect_list(\"leaf_cert.not_before\").alias(\"not_befores\")).select(\"all_domains\", filter_duration_udf(psf.col(\"not_befores\")).alias(\"befores\")).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Table to see how long certificates are being renewed for. Check the first and last date. Accuracy is limited accounted to the limited timeframe in relation to a domain renewal of minimum 1 year)\n",
    "\n",
    "def first_tup(obj):\n",
    "    first = min(obj)\n",
    "    a = time.gmtime(first)\n",
    "    return a.tm_year*100+a.tm_mon\n",
    "\n",
    "def last_tup(obj):\n",
    "    last = max(obj)\n",
    "    a = time.gmtime(last)\n",
    "    return a.tm_year*100+a.tm_mon\n",
    "\n",
    "first_tup_udf = psf.udf(lambda x: first_tup(x), pst.IntegerType())\n",
    "\n",
    "last_tup_udf = psf.udf(lambda x: last_tup(x), pst.IntegerType())\n",
    "\n",
    "filtered_df.withColumn(\"counts\", slen(psf.col(\"befores\"))).withColumn(\"first_year_month\", first_tup_udf(psf.col(\"befores\"))).withColumn(\"last_year_month\", last_tup_udf(psf.col(\"befores\"))).groupBy(\"counts\", \"first_year_month\", \"last_year_month\").count().sort(psf.desc(\"last_year_month\")).sort(psf.desc(\"first_year_month\")).sort(psf.desc(\"count\")).show(1000, truncate=False)\n",
    "# First column: Amount of renewals\n",
    "# Second column: First timestamp for certificate\n",
    "# Third column: Last timestamp for certificate\n",
    "# Fourth column: Total amount of occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up udf to calculate the duration/interval between certificates.\n",
    "\n",
    "def duration(obj):\n",
    "    obj.sort()\n",
    "    np.interval(obj)\n",
    "    \n",
    "    [abs(j-i) for i,j in zip(x, x[1:])]\n",
    "    \n",
    "duration_udf = psf.udf((lambda x: [abs(j-i) for i,j in zip(x, x[1:])] if len(x) > 1 else None), pst.ArrayType(pst.FloatType()))\n",
    "\n",
    "# Set up the dataframe for the interval between certificates. Make persistant since this is often re-used.\n",
    "interval_df = filtered_df.where(psf.size(psf.col(\"befores\")) >= 2).withColumn(\"intervals\", duration_udf(psf.col(\"befores\"))).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find overall duration across all certificates\n",
    "exploded_df = interval_df.select(\"all_domains\", psf.explode(psf.col(\"intervals\")).alias(\"duration\"))\n",
    "# exploded_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a table with all durations grouped together in amount of days (1 hour in seconds * 24 hours)\n",
    "# Sorted by amount of occurrences\n",
    "exploded_df.withColumn(\"duration\", psf.round(psf.col(\"duration\")/(3600*24))).groupBy(\"duration\").count().sort(psf.desc(\"count\")).show(1000, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a table with all durations grouped together in amount of days (1 hour in seconds * 24 hours)\n",
    "# Sorted by amount of days\n",
    "exploded_df.withColumn(\"duration\", psf.round(psf.col(\"duration\")/(3600*24))).groupBy(\"duration\").count().sort(psf.desc(\"duration\")).show(1000, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pandas dataframe of the pyspark dataframe for local processing (generating the graph)\n",
    "exploded_pdf = exploded_df.withColumn(\"Interval\", psf.round(psf.col(\"duration\")/(3600*24))).groupBy(\"Interval\").count().sort(psf.asc(\"Interval\")).where(psf.col(\"Interval\")<400).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph\n",
    "a = exploded_pdf.plot(x='Interval', y='count', x_compat=True, logy=True, title=\"All certificates, log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up udf for getting the mean of the interval for each group of Common Names\n",
    "array_median = psf.udf(lambda x: float(np.median(x)), pst.FloatType())\n",
    "# Make a pandas dataframe for local processing\n",
    "median_pdf = interval_df.withColumn(\"Median\", psf.round(array_median(psf.col(\"intervals\"))/(3600*24))).groupBy(\"Median\").count().sort(psf.desc(\"Median\")).where(psf.col(\"Median\")<110).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph\n",
    "median_plot = median_pdf.plot(x=\"Median\", y=\"count\",  logy=True, title=\"Grouped median, log\", x_compat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up udf for getting the average of the interval for each group of Common Names\n",
    "array_average = psf.udf(lambda x: float(np.average(x)), pst.FloatType())\n",
    "# Make a pandas dataframe for local processing\n",
    "avg_pdf = interval_df.withColumn(\"Average\", psf.round(array_average(psf.col(\"intervals\"))/(3600*24))).groupBy(\"Average\").count().sort(psf.desc(\"Average\")).where(psf.col(\"Average\")<110).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph\n",
    "avg_plot = avg_pdf.plot(x=\"Average\", y=\"count\",  logy=True, title=\"Grouped average, log\", x_compat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
